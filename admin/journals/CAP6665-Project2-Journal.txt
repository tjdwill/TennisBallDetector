CAP6665 Project 2 Journal


=============
14 July 2023
=============

- Changed "processing_helpers.py" to now output a default angle (360) to signal that the input list was empty.
- Refactored BallDetector class in ss02_BallDetector 
    - Made resetting the detection infrastructure a function.
    - Added a check for the 360 degree angle result.
- Tested the program with the new tuner setup. Tuner sets params, and ss02 is able to get them.


- Noticed an odd behavior regarding subsequent detections. The plot shows detection points that are outliers. These points may be due to camera shifting after waking from sleep, but I don't understand how that would be possible since the physical device doesn't move. It also isn't due to old images because the queue size is 1. 
    - It is to be noted, however, that resetting the detection count midway (I obscure the camera with an object and then remove it) does result in normal behavior. No erratic points, so I am thinking it does have something to do with the first few images processed on a post-move detection.
    
- Did some investigating between calculated angle and the factor needed to actually align with the ball. So far, for angles greater than 50 in magnitude, the factor seems to be around 2. I will run a test where I half the angle for large values and see if the JH can align with the ball in one pass.

 
=============
13 July 2023
=============

- Updated "cameraTuner.py" to be a true ROS node. It now loads a default parameter configuration and updates the ROS Parameter Server upon the program exiting. 
- In light of the "cameraTuner.py" changes, "ss02_BallDetector" was updated to get its image processing parameters from the ROS Parameter Server.

=============
12 July 2023
=============
Went to ISR Lab and observed the ball detection in that environment. The lighting led to changes in S_min and V_min in the HSV parameters. Blurs and contrast were the same though.


I'm considering making the cameraTuner.py file a separate program within the package that will allow the user to tune the camera and upload the parameters to the Parameter Server before a run of the detection program. This will allow the program to be more adaptive and will save space on using different sets of configurations. 

Also, I created an ad-hoc solution to the angle problem. I just reduce the calculate angle by a factor of five. Over multiple detection cycles, the robot converges to the ball. I can still pursue better methods (of course), but this will work for video purposes.

=============
10 July 2023
=============
Updated report to include ROS implementation details and Results. 

===========
9 July 2023
===========
::Code editing::
- Re-wrote MoveRobot.srv to be an action MoveRobot.action
- Implemented ss04_RobotController
- Created tw_tennis.launch
- ss03_DetectionProcessor: implemented SimpleActionClient logic.


I think I will have to rewrite the code such that only ss02 is a SimpleActionClient. Currently, it goes ss02->ss03->ss04 in nested processes, but I would be more comfortable if it goes ss02->ss03; ss02->ss04. The code will still block ss02 from detecting, but this way I won't have a nested process, ss03 won't be SimpleActionServer and a SimpleActionClient, and ss02 will be SimpleActionClient for two SimpleActionServers. I will have to adjust the ProcessDetection.action file to return the calculated angle as its result.


- Just rewrote the ss02 node to be a SAC for ss04 in addition to ss03. I also reorganized the code to run from a class; it's now much neater in my opinion.
    - Changed ss01 to display uninterrupted live feed; Useful for when ss02 goes into blocking and gives a purpose to images captured during this time period.
    - ss02 houghCircles method to only display the blurred image if we internally toggle debug mode. I did this to keep the screen from becoming cluttered with too many outputs (similar to that of ss03's Matplotlib debug option).
    - Added an additonal ROS Paramter for DETECTION_THRESH. If the programs can successfully grab the parameters from the Parameter Server, I will remove the relevant hard-coded values.
- I was able to get the ROS Parameter Server version working. The JetHexa is now able to detect balls, cal. angles, and move. Problem is, it keeps moving past the ball (which is what I feared). Spherical Geometry, it is. Hopefully, that will work for me.
- Also, I need to figure out how long to block for the robot to finish its move. Right now, I'm truing 2x the number of movement cycles it needs to take since each movement cycle is ~1 second long.


===========
7 July 2023
===========
I had quite the scare this morning. I thought I (somehow) removed the jethexa packages from the system. Before, I'd tried to build the numpy_msgs package with the custom messages and helper module, but I kept receiving an ImportError message regarding a Python module called 'em'. Even though the module was on the system, it wouldn't import. answer.ros.org residents say it is because 'em' conflicts with another package called "empy". What I had to do was the processo outlined 

https://answers.ros.org/question/257331/python-module-empy-missing-tutorials/



1. source ros from /opt/ros/kinetic/setup.zsh  (melodic in my case)

2. clean up your catkin workspace (except src)

   $ cd ~/catkin_ws
   $ unlink src/CMakeLists.txt
   $ rm -rf build
   $ rm -rf devel
   $ rm .catkin_workspace

3. Re-initialize the workspace with catkin_init_workspace from inside src
4. Run catkin_make

The problem was that I didn't realize that "jethexa" was its own catkin workspace, so sourcing ROS's setup.zsh without subsequently sourcing jethexa/devel/setup.zsh resulted in the jethexa workspace being invisible to ROS. After sourcing the setup file and then rebuilding MY workspace with catkin_make, the build completed successfully, and ROS could see JetHexa packages.


I also learned that roslaunch automatically begins a roscore if it hasn't, which is why I thought Hiwonder had a way to start roscore in the shell automatically. That's why rosrun would not work without calling roscore in a separate shell first.

We got everything working so far, PY.


---
Finished subsystems 01-03 (Photographer, BallDetector, and DetectionProcessor). Had to update the libraries numerous times, but now it is working as expected. PY
Looking forward to integrating subsystem04 (JetHexa Controller) to begin testing and revision.

Also, a portion of the rewriting of the module(s) were due to Python 2.7 restrictions. Features I currently take as a given are not present in that version, so the code has to be backwards compatible.


===========
6 July 2023
===========
- Did some much-needed report writing. Wrote all about the solution to the noise problem. I'm glad I started the report today because doing it all at once would have been dreadful honestly.

- Conducted more tests on the JetHexa's motion controller. None of the tutorials (to my knowledge) instruct the user on how to make the robot move through a specifc angle or distance. It seems the controller is meant for more continuous-based movement though a finite number of steps can be specified.

Moreover, the parameters in the traveling [sic] function have different functions than stated. For example,
	- rotation is actually the angle (in radians) the robot moves through per *cycle* (Hiwonder -> me, step -> movement cycle; stride -> one leg tep).
	- time is the amount of time a cycle takes to complete (affects rotation speed for example)
	- steps is the number of complete movement cycles.
	
What I also found was that the robot doesn't rotate through the angle one would expect when over a certain to-be-determined value. For example, 
			(rotation=pi/10;  time= 1.0; steps = 10) should result in a 180 degree movement over the course of 10 seconds. In reality, it was about 325. At the very least it was well over 180 (and 270). However, (rotation=pi/20 [9 degrees]; time= 1.0; steps=11 [yes, 11]) resulted in 180 rotation to my confusion.
	
At a rotation "speed" of 1 degree per cycle (pi/180); time=1.0, steps=180; I get the desired motion. The problem, however, is that it takes 1 second per cycle. I can bump it down to 0.5s per cylce, but that would still be lengthy. I may try 2, 4, 6, and 8 degrees per cycle. I tried 10 and got the exact same result as 9 degrees.; it took 11 steps.  

This likely means I have lower resolution for the angle movements. That shouldn't be too troublesome, however, because the angles don't have to be exact in order to produce a good alignment with a detected tennis ball. This is likely going to be a trial-and-error type of deal.

I tried reading through the source code again to see if I could find answers for this behavior, but it did not yield desirable results. For one, I am not at the level of programming skill to deeply understand a system as complex as the JetHexa; it's really a great piece of software engineering in my inexperienced eyes. Additionally, however, there is an imported module called "kinematics" that leads me to a dead end because I can't find it. I don't know where the module comes from, and online searches don't seem to either, unfortunately.

- Create templates for the project's ROS packages.
===========
5 July 2023
===========
Today was primarily a research day in the sense that I spent time learning about ROS aActions and how they work. After gaining a basic ugrasp of the communnication method, I followed a few tutorials to write SimpleActionClient and SimpleActionServer. To test that the data was transported correctly, I printed the data (numpy arrays) from both the server-side and the clientside. The results were quite pleasing, both versions matching identically. 

The purpose of the test was to ensure actions would be a viable communication option for the second and thrid subsystems fof the project (ball detector and data processing unit, respectively_. Now, not only do I have a means of sending the data and allowing for a long recipient callback, I can block the ballDetector from taking in new input until the data processor is finished with the data. This solves my problem regarding synchronization for now.

UPDATE:
So I was wrong (completely) about the JetHexa control. I need to use the jethexa_controller.client.Client() method in order to attain the desired motion. The other method results in the robot twisting as if one were to look over his or her shoulder. Using the client method results in in-place motion. To do so, set the stride to zero in the traveling [sic] method call.

Also, I may do something rather unorthodox to implement the project. One of my nodes may need to be a subscriber, SimpleActionServer, and a ServiceClient. I'm not sure if that is a huge error in terms of communication, but I truly think this may be a necessary course-of-action. I am unable to think of a way to cleanly segment the nodes, and I won't invent a reason to do so. We'll see if it works.

===========
4 July 2023
===========
	- Studied the JetHexa source code and example code to review how the robot operates and how I may control it. What I've learned was that there are two primary methods of controlling the robot via the API:
		When the robot actually moves out of place, HiWonder's example programs use the client from the jethexa_controller package to send commands via the tupical publisher-subscriber method. The client object instantiates an instance of the jJetHexa object that servesas the primary controll for the robot. It listens on the topic and coordinates the necessary movements to perform the action.
		
		For programs that only move in-place, the client is not imported. Instead, the jethexa module is imported directlyu, and it's JetHexa object is instantiated. Then, this object's transform_pose_2 method is used to alter the robot's pose via rotation or in-place translation.

		Since my goal is to rotate the robot in place, I will use the latter method. As a bonus, unlike the use of the other method, I do not have to add an "include" to my .launch file.

 -  Revised the implementation of the custom ROS Messages. 
	Now called ROSNumpy and ROSNumpyList, respectively. The former still  takes a float64[] but also has an int16[] for the ndarray shape and 'string' for the dtype. As a result, we can (in theory) deconstruct and reconstruct any numpy array like so:
	
	Sources:
	https://numpy.org/doc/stable/reference/arrays.interface.html#arrays-interface
	
	
	> arr = np.random.randint(0, 100, size(4, 3)  # Example array
	> msg = ROSNumpy()
	> msg.dtype = arr.dtype.name  # A string rep. of the array's dtype
	> msg.shape = arr.shape
	> msg.rosnp = arr.reshape(-1)  # completely flatten array.
	
	This works because, even though the abstraction allows us to think in multi-dimensional array depictions, the array is physically represented in memory as a single-contiguous array. We're just reducing and then restoring the level of abstraction.
	
	
	I wrote helper functions to assist with msg construction and deconstruction, allowing the use of map() and/or list comprehension (LC) to increase readibility.
	
	::For multiple arrays (list of numpy arrays)::
	*Assume they've all been converted to ROSNumpy messages* (Loop, LC, etc.)
	
	> arr_list = [msg1, msg2, ...]
	>arr_msg = ROSNumpyList(arr_list)
	> pub.publish(arr_msg)
	
	To retrieve the data  (unpack and reconstruct):
	*From ROSNumpyList* Here is an example of retrieving the first array (step-by-step):
	> received_list = msg.rosnp_list
	> first_array = received_list[0]
	> first_shape = first_array.shape
	> first_dtype = first_array.dtype
	> first_array  = first_array.rosnp
	# Reconstruct the original array
	> first_array = np.array(first_array, dtype=first_dtype).reshape(first_shape)
	
	
===========
3 July 2023
===========

::ROS Work::
- Experimented with Matplotlib on a ROS pub/sub dynamic. Attempted to find matplotlibrc file, but could not locate it on my VM. Checked ~/.config, and various /usr sub-directories. Note that because MPL currently uses Tkinter on Linux, it will force a block for interactive plotting that renders rospy.spin() useless. 

- Implemented a means to send an array of Numpy arrays as a message (Numpy64List). ROS apparently wants the users to define use-case specific message rather than rely on standard messages (especially for semantically-relevant field names), so I used the fact that Numpy's multi-dimensionality is simply an abstraction to my advantage by flattening the arrays to 1-D. The message receiver can then unpack the messages and reconstruct the original arrays (we know the inner-most channel has 3 elements). If desired, perhaps I can store each array's shape as a list (or pre-allocated array?) to allow reconstruction that is more robust.


To-Do for this week, 
	- Study the JetHexa's controller (review how it works); How can you pass an angle to it to get the desired rotation?
	- 

Meeting w/ Prof. Sevil

1. What's the best way to choose the proper cluster? Currently, "vote" has an edge over "scoring" as it is able ot more accurately detect edge cases. However, there is a downside that any cluster with significantly higher density is chosen, even if another valid cluster is much closer. 
	A: Trust the numbers; Use trials to compare the results and choose based on that. Be sure to test in multiple environments. (inside, outside, diff. room, etc.)
	
2.  Is it unwise to get rid of radii estimates? What if I need to account for the balls' radii?
	A: If it would ruin your momentum to restructure the program, don't worry about it right now.
	
7. Angle Calculation: How to convert pic angle to real angle about z axis? 
	A. We don't need to; just place the angle as a yaw angle. Test to ensure it works.
===========
29 June 2023
===========

- Adjusted classes from imgClass repo to help with testing.
- Adjusted "acceleration" debug function to plot with the proper x-y alignment. 
- Routine print statement removal for clarity. 
- Implemented corrective rotation based on calculated angle.
	- More on this: I think I was correct in thinking the story is incomplete on this front. Currently, I am calculating an angle that would result in a rotation about the x-axis ("roll"). This makes sense because the image is 2D. However, the robot would rotate about the z-axis, so how do I determine the proper angle about the z-axis (I believe that's the "yaw" angle). 
		- Do I need to look into spherical trigonometry?

===========
28 June 2023
===========

Discovered the volatility of density calculation due to a single outlier point. Currently attempting to remedy the problem. 
	- Investigating distance "acceleration": how quickly the change in change in distance occurs. Perhaps this can result in the creation of a boundary between inliers and outliers.
	- TO find point with max acceleration
		index = np.nonzero(np.equal(cluster1acc, max(cluster1acc)))[0]
		
		Correspo. Distance:
		index_num = index[0]  # Get the numeric form
		distance_With_max_accel = cluster1dist[index_num+2]
	(UPDATE): Added a check during distance calculation. If the acceleration is above a threshold value (currently 5), the program gets the index mentioned above and uses it to get the distance value at that index. This results in getting the distance two indices before the outlier point. (Remember that if point X has the highest acceleration at pointAccel[m], the corresponding distance is at pointDist[m+2]. Therefore, pointDist[m] is two units before point X.
	
	Initial tests have proved this to be a promising adjustment.
	
::cv_helpers module::
	Stylistic editing to look more professional.
	TO-DO: write tests to ensure proper functionality

::Questions::
	1. What's the best way to choose the proper cluster? Currently, "vote" has an edge over "scoring" as it is able ot more accurately detect edge cases. However, there is a downside that any cluster with significantly higher density is chosen, even if another valid cluster is much closer. 
	
	2. Should there be an absolute check for density? As in, do I define a threshold over which a ball is *extremely* likely? The upside of prefering density is that you bias toward more sure ball detections. The "collection" pattern could just be density-biased instead of distance biased. Overtime, the balls will likely still be collected.
	
	3. If I do improve "score", how should I do it?
	
	4. Currently, the acceleration threshold is 5, and the program picks the distance associated two data points away. This is a result of the calculation for acceleration. If the point of max acceleration has index j. The corresponding distance point is j+2. Passing j into the distance array will get the point 2 units to the left of j (which is guaranteed to exist since j exists in the acceleration array). Is there an even better method for dealing with outliers that is accessible and approachable?
	
	5. Currently, the circle detection sees considerable improvement after making adjustments to contrast and doing a triple-Mean+Gauss filtering. Is this viable?

	6. Is it unwise to get rid of radii estimates? What if I need to account for the balls' radii?
	
	7. Angle Calculation: How to convert pic angle to real angle about z axis? 
	
===========
27 June 2023
===========
TO-DO: Research machine learning modules; what would be best for my needs?
	- Detectron2 is out; I don't have the hardware for it.
	- Google Colab?

KMeans:
	- Added label and legend to plot
	
Cluster Filtering:
	- Added Minimum Cluster Point Count Filtering
	- Added Cluster Density calculation
	- Added Cluster-Ground Pixel Distance calculation
	
===========
26 June 2023
===========
- Revised angle calulation function to allow for desired input; points will now be input in [row_index, col_index] order.
	-  Revised corresponding test
-  Wrote data pre-processing function. Given a list of circle center arrays, it removes the radii information and reorders the remaining elements
	for the proper order.

- In preparation for k-Means segmentation, modified my KMeans class to accept initial means.


===========
25 June 2023
===========

-Wrote function that allow the user to calculate the angle formed by a vertical and a vector made from two pixels.
	Currently, you have to place the target point in reverse order [x,y] rather than the desired [y, x] for image indexin	g.
		- Also wrote a test to vaiidate the results. 
	
- Wrote a function that determines the "ground pixel"; the pixel used to form vectors with other pixels. It is the bottom-center pixel of an image.


